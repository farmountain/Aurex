# ðŸ§  AUREX: Agentic Unified Runtime for Execution

> An open-source, cross-platform AI acceleration framework that unifies **tensor**, **symbolic**, and **procedural** execution for **AI agents**, built to outperform CUDA and democratize access to advanced compute across AMD, Intel, and heterogeneous systems.

---

## ðŸš€ Why AUREX?

Most frameworksâ€”like CUDAâ€”optimize solely for matrix math and NVIDIA GPUs. But the next generation of intelligence demands:

- Reasoning agents, not just neural nets
- Symbolic control flow alongside tensor math
- Reflexive adaptation, energy-efficiency, and hardware portability

**AUREX** is designed from the ground up to support **AI agents**, **LLMs**, **procedural planners**, and **symbolic memory systems**, while running on **AMD**, **Intel**, **CPU**, **GPU**, **NPU**, and even edge devices.

---

## ðŸ§  Agent-Native Execution Stack

[ AUREX Runtime (AUREUS Steering: EffortEvaluator, ReflexionLoop, ConfidenceRegulator) ]
              â†“             â†“             â†“               â†“
 [ Tensor Kernels ] [ Symbolic FSM Ops ] [ Procedural Ops ] [ Agent Scheduler ]
              â†“
   [ LLVM / MLIR IR Compiler with Passes ]
              â†“
   [ ROCm | SYCL | Vulkan | CPU Backend ]

---

## ðŸ§© Core Modules

### ðŸ”¢ Tensor Kernels
- MatMul, Attention, Convolution, LayerNorm
- Quantization (int8, bf16, fp16)
- Kernel fusion & dynamic graph optimization

### ðŸ§  Symbolic + Procedural Execution
- Finite State Machine (FSM) execution for rule-based agents
- Procedural reasoning & goal-subgoal graphs
- Temporal memory buffer + hypothesis branching

### âš›ï¸ Agent Runtime (AUREUS Core)
- `EffortEvaluator`: energy/task efficiency routing
- `ConfidenceRegulator`: dynamic precision scaling
- `ReflexionLoop`: runtime graph rewrites
- `HypothesisManager`: plan exploration & rollback

### ðŸ”Œ Plugin + Backend Abstraction
- Multi-device execution via:
  - AMD ROCm
  - Intel SYCL (OneAPI)
  - Vulkan compute
  - CPU (fallback)
- Plugin system for custom ops, NPU drivers, edge runtimes

### Aurex-AMDUDA Core Runtime

The `amduda` crate provides the core CUDA-alternative runtime targeting AMD/Intel GPUs and CPU SIMD. It includes a unified TensorOps API, procedural kernel scheduler, multi-tier memory manager, and standalone LLM inference engine with optional integrations.

---

## ðŸ§ª Example: LLM Agent on AMD ROCm

```python
from aurex.runtime import Agent, load_model

class MyAgent(Agent):
    def perceive(self, text):
        return self.tokenizer.encode(text)
    
    def reason(self, tokens):
        return self.model.forward(tokens)  # backend auto-routed

    def act(self, output):
        return self.tokenizer.decode(output)

agent = MyAgent(model=load_model("phi-3", backend="rocm"))
agent.run("Plan a trip to Kyoto with hidden gems.")


---

ðŸŽ¯ Competitive Edge vs CUDA

Feature	NVIDIA CUDA	AUREX

Open Hardware	âŒ NVIDIA-only	âœ… AMD, Intel, CPU, NPU
Agent Loop	âŒ No	âœ… Yes (Reflexion, FSM, Reasoning)
Symbolic Support	âŒ	âœ… FSM, Rule Engines
Procedural Ops	âŒ	âœ… Temporal, Stack Ops
Runtime Steering	âŒ	âœ… AUREUS EffortEvaluator
Mobile/Edge	âš ï¸ Limited	âœ… Targeted
Licensing	Proprietary	MIT/BSD Open Source



---

ðŸ“¦ Installation (Coming Soon)

pip install aurex-runtime
aurex-cli compile my_model.py --target=rocm

> Note: ROCm 6.0+ or SYCL-enabled devices required. Vulkan and CPU fallback supported.




---

ðŸ”¬ Roadmap

Phase	Milestones

Phase 1	Tensor core kernels (MatMul, Conv, Attn), ROCm + SYCL backend, CLI
Phase 2	Agent runtime, symbolic kernel support, profiling tools
Phase 3	Edge deployment (WASM, ARM), plugin marketplace, full AI agent ecosystem



---

ðŸ§‘â€ðŸ’» Contributing

We welcome contributors! Please see CONTRIBUTING.md to get started.

Join the mission to free AI from vendor lock-in and build truly intelligent, self-reasoning agents that run everywhere.


---

ðŸ§­ Philosophy

> "The future of AI is not just tensors â€” itâ€™s decisions, reasoning, memory, and intent.
AUREX is built to serve the next wave of AI: agents that think, reflect, and act across any platform."



â€” AUREX Maintainers


---

ðŸ“„ License

MIT License Â© 2025 AUREX Project Contributors

---
